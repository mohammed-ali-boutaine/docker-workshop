# Workshop ETL : Dockerisation d'une application Airflow + FastAPI + Streamlit

## ğŸ¯ Objectif du Workshop

Apprendre Ã  dockeriser une application ETL complÃ¨te composÃ©e de :
- **Airflow** : Orchestration et gestion du pipeline ETL
- **FastAPI** : API REST pour exposer les donnÃ©es
- **Streamlit** : Interface de visualisation des donnÃ©es
- **PostgreSQL** : Base de donnÃ©es relationnelle
- **MongoDB** : Base de donnÃ©es NoSQL

---

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose installÃ©s
- Connaissances de base en Python
- Ã‰diteur de code (VS Code recommandÃ©)
- Les fichiers sources de l'application (airflow/, fastapi/, streamlit/)

---

## ğŸ“ Structure du Projet Ã  CrÃ©er

```
docker-workshop/
â”‚
â”œâ”€â”€ docker-compose.yml          # Ã€ crÃ©er : orchestration de tous les services
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Ã€ crÃ©er : image Airflow personnalisÃ©e
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ etl_dag.py          # DÃ©jÃ  fourni : DAG pour ETL
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ input.csv           # DÃ©jÃ  fourni : donnÃ©es d'entrÃ©e
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ Dockerfile              # Ã€ crÃ©er : image FastAPI
â”‚   â”œâ”€â”€ main.py                 # DÃ©jÃ  fourni : code de l'API
â”‚   â””â”€â”€ requirements.txt        # DÃ©jÃ  fourni : dÃ©pendances
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ Dockerfile              # Ã€ crÃ©er : image Streamlit
â”‚   â”œâ”€â”€ app.py                  # DÃ©jÃ  fourni : code de l'app
â”‚   â””â”€â”€ requirements.txt        # DÃ©jÃ  fourni : dÃ©pendances
â””â”€â”€ README.md
```

---

## ğŸš€ Ã‰tape 1 : CrÃ©er le Dockerfile pour Airflow

### Objectif
CrÃ©er une image Docker personnalisÃ©e pour Airflow qui inclut toutes les dÃ©pendances nÃ©cessaires.

### Instructions

1. CrÃ©ez le fichier `airflow/Dockerfile`

2. Contenu du Dockerfile :

```dockerfile
# Image de base Airflow
FROM apache/airflow:2.7.3-python3.11

# Passer en mode root pour installer des packages systÃ¨me
USER root

# Installer les dÃ©pendances systÃ¨me nÃ©cessaires
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Revenir Ã  l'utilisateur airflow
USER airflow

# Copier et installer les dÃ©pendances Python
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# DÃ©finir le rÃ©pertoire de travail
WORKDIR /opt/airflow
```

3. CrÃ©ez le fichier `airflow/requirements.txt` :

```txt
pandas>=2.0.0
psycopg2-binary>=2.9.0
pymongo>=4.0.0
sqlalchemy>=2.0.0
```

### ğŸ“ Points ClÃ©s Ã  Expliquer

- **FROM** : Utilise l'image officielle Airflow
- **USER root/airflow** : Changement d'utilisateur pour les permissions
- **RUN apt-get** : Installation de dÃ©pendances systÃ¨me
- **COPY requirements.txt** : Copie des dÃ©pendances Python
- **WORKDIR** : DÃ©finit le rÃ©pertoire de travail dans le conteneur

---

## ğŸš€ Ã‰tape 2 : CrÃ©er le Dockerfile pour FastAPI

### Objectif
CrÃ©er une image Docker lÃ©gÃ¨re pour l'API FastAPI.

### Instructions

1. CrÃ©ez le fichier `fastapi/Dockerfile`

2. Contenu du Dockerfile :

```dockerfile
# Image de base Python lÃ©gÃ¨re
FROM python:3.11-slim

# DÃ©finir le rÃ©pertoire de travail
WORKDIR /app

# Copier les dÃ©pendances
COPY requirements.txt .

# Installer les dÃ©pendances
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code de l'application
COPY . .

# Exposer le port de l'API
EXPOSE 8000

# Commande de dÃ©marrage
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

3. VÃ©rifiez que `fastapi/requirements.txt` contient :

```txt
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
psycopg2-binary>=2.9.0
pymongo>=4.0.0
sqlalchemy>=2.0.0
pydantic>=2.0.0
```

### ğŸ“ Points ClÃ©s Ã  Expliquer

- **python:3.11-slim** : Image Python lÃ©gÃ¨re
- **WORKDIR /app** : Tous les fichiers seront dans /app
- **EXPOSE 8000** : Documente le port utilisÃ©
- **uvicorn --reload** : Recharge automatique en cas de modification

---

## ğŸš€ Ã‰tape 3 : CrÃ©er le Dockerfile pour Streamlit

### Objectif
CrÃ©er une image Docker pour l'interface de visualisation Streamlit.

### Instructions

1. CrÃ©ez le fichier `streamlit/Dockerfile`

2. Contenu du Dockerfile :

```dockerfile
# Image de base Python
FROM python:3.11-slim

# DÃ©finir le rÃ©pertoire de travail
WORKDIR /app

# Copier les dÃ©pendances
COPY requirements.txt .

# Installer les dÃ©pendances
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code de l'application
COPY . .

# Exposer le port Streamlit
EXPOSE 8501

# Commande de dÃ©marrage
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

3. VÃ©rifiez que `streamlit/requirements.txt` contient :

```txt
streamlit>=1.28.0
pandas>=2.0.0
psycopg2-binary>=2.9.0
pymongo>=4.0.0
sqlalchemy>=2.0.0
plotly>=5.17.0
```

### ğŸ“ Points ClÃ©s Ã  Expliquer

- **EXPOSE 8501** : Port par dÃ©faut de Streamlit
- **--server.address=0.0.0.0** : Permet l'accÃ¨s depuis l'extÃ©rieur du conteneur
- **streamlit run** : Commande pour dÃ©marrer l'application

---

## ğŸš€ Ã‰tape 4 : CrÃ©er le fichier docker-compose.yml

### Objectif
Orchestrer tous les services et dÃ©finir leurs interactions.

### Instructions

1. CrÃ©ez le fichier `docker-compose.yml` Ã  la racine du projet

2. Contenu du docker-compose.yml :

```yaml
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: postgres_db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - etl_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MongoDB Database
  mongodb:
    image: mongo:7.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_DATABASE: etl_db
    volumes:
      - mongodb_data:/data/db
    networks:
      - etl_network

  # Airflow Webserver
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=secret
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/data:/opt/airflow/data
      - airflow_logs:/opt/airflow/logs
    networks:
      - etl_network
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      bash -c "airflow db init &&
               airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
               airflow webserver"

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/data:/opt/airflow/data
      - airflow_logs:/opt/airflow/logs
    networks:
      - etl_network
    depends_on:
      postgres:
        condition: service_healthy
    command: airflow scheduler

  # FastAPI Service
  fastapi:
    build:
      context: ./fastapi
      dockerfile: Dockerfile
    container_name: fastapi_app
    ports:
      - "8000:8000"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
      - MONGODB_HOST=mongodb
      - MONGODB_PORT=27017
    networks:
      - etl_network
    depends_on:
      - postgres
      - mongodb

  # Streamlit Service
  streamlit:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: streamlit_app
    ports:
      - "8501:8501"
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
      - MONGODB_HOST=mongodb
      - MONGODB_PORT=27017
    networks:
      - etl_network
    depends_on:
      - postgres
      - mongodb

volumes:
  postgres_data:
  mongodb_data:
  airflow_logs:

networks:
  etl_network:
    driver: bridge
```

### ğŸ“ Points ClÃ©s Ã  Expliquer

#### Services
- **postgres** : Base de donnÃ©es pour Airflow et les donnÃ©es ETL
- **mongodb** : Base de donnÃ©es NoSQL pour stockage alternatif
- **airflow-webserver** : Interface web d'Airflow
- **airflow-scheduler** : Planificateur des DAGs
- **fastapi** : API REST
- **streamlit** : Interface de visualisation

#### Configuration importante
- **depends_on** : DÃ©finit l'ordre de dÃ©marrage
- **healthcheck** : VÃ©rifie que PostgreSQL est prÃªt
- **networks** : Permet la communication entre conteneurs
- **volumes** : Persiste les donnÃ©es et partage les fichiers
- **environment** : Variables d'environnement pour la configuration

#### Ports exposÃ©s
- `5432` : PostgreSQL
- `27017` : MongoDB
- `8080` : Airflow Web UI
- `8000` : FastAPI
- `8501` : Streamlit

---

## ğŸš€ Ã‰tape 5 : PrÃ©parer les donnÃ©es d'entrÃ©e

### Instructions

1. CrÃ©ez le fichier `airflow/data/input.csv` avec des donnÃ©es de test :

```csv
id,name,value,category
1,Product A,100,Electronics
2,Product B,200,Clothing
3,Product C,150,Food
4,Product D,300,Electronics
5,Product E,50,Clothing
```

---

## ğŸš€ Ã‰tape 6 : Lancer l'application

### Instructions

1. **Construire et dÃ©marrer tous les services** :

```bash
docker-compose up -d --build
```

2. **VÃ©rifier que tous les conteneurs sont dÃ©marrÃ©s** :

```bash
docker-compose ps
```

3. **VÃ©rifier les logs en cas d'erreur** :

```bash
# Logs de tous les services
docker-compose logs

# Logs d'un service spÃ©cifique
docker-compose logs airflow
docker-compose logs fastapi
docker-compose logs streamlit
```

---

## ğŸš€ Ã‰tape 7 : Tester l'application

### 1. Airflow (http://localhost:8080)

- **Connexion** : `admin` / `admin`
- **Activer le DAG** : Recherchez `csv_to_postgres` et activez-le
- **Trigger manuel** : Cliquez sur le bouton "Play" pour exÃ©cuter le DAG
- **VÃ©rifier les logs** : Cliquez sur les tÃ¢ches pour voir leur exÃ©cution

### 2. FastAPI (http://localhost:8000)

- **Documentation Swagger** : http://localhost:8000/docs
- **Tester l'endpoint** : 
  - GET `/data` : RÃ©cupÃ©rer toutes les donnÃ©es
  - GET `/health` : VÃ©rifier l'Ã©tat de l'API

### 3. Streamlit (http://localhost:8501)

- **Interface de visualisation** : Affiche les graphiques et tableaux
- **Actualisation** : Recharge automatiquement les donnÃ©es

### 4. PostgreSQL

Connexion via CLI :

```bash
docker exec -it postgres_db psql -U airflow -d airflow
```

VÃ©rifier les donnÃ©es :

```sql
\dt                          -- Lister les tables
SELECT * FROM etl_data;      -- Voir les donnÃ©es
```

### 5. MongoDB

Connexion via CLI :

```bash
docker exec -it mongodb mongosh
```

Commandes MongoDB :

```javascript
show dbs
use etl_db
show collections
db.etl_data.find()
```

---

## ğŸ”§ Commandes Docker Utiles

### Gestion des conteneurs

```bash
# DÃ©marrer les services
docker-compose up -d

# ArrÃªter les services
docker-compose down

# ArrÃªter et supprimer les volumes (âš ï¸ supprime les donnÃ©es)
docker-compose down -v

# Reconstruire les images
docker-compose build

# Reconstruire et redÃ©marrer
docker-compose up -d --build

# Voir les logs en temps rÃ©el
docker-compose logs -f

# RedÃ©marrer un service spÃ©cifique
docker-compose restart airflow
```

### DÃ©bogage

```bash
# AccÃ©der au shell d'un conteneur
docker exec -it airflow bash
docker exec -it fastapi_app bash
docker exec -it streamlit_app bash

# Inspecter un conteneur
docker inspect airflow

# Voir l'utilisation des ressources
docker stats

# Nettoyer les ressources non utilisÃ©es
docker system prune -a
```

---

## ğŸ› RÃ©solution des ProblÃ¨mes Courants

### ProblÃ¨me 1 : Airflow ne dÃ©marre pas

**SymptÃ´me** : Le conteneur Airflow redÃ©marre en boucle

**Solutions** :
1. VÃ©rifier que PostgreSQL est bien dÃ©marrÃ© :
   ```bash
   docker-compose logs postgres
   ```

2. VÃ©rifier les logs Airflow :
   ```bash
   docker-compose logs airflow
   ```

3. RÃ©initialiser la base de donnÃ©es :
   ```bash
   docker-compose down -v
   docker-compose up -d
   ```

### ProblÃ¨me 2 : FastAPI ne se connecte pas Ã  PostgreSQL

**Solutions** :
1. VÃ©rifier les variables d'environnement dans docker-compose.yml
2. S'assurer que le rÃ©seau est bien configurÃ©
3. Tester la connexion :
   ```bash
   docker exec -it fastapi_app ping postgres
   ```

### ProblÃ¨me 3 : Le DAG ne trouve pas le fichier CSV

**Solutions** :
1. VÃ©rifier que le volume est bien montÃ© :
   ```bash
   docker exec -it airflow ls -la /opt/airflow/data
   ```

2. VÃ©rifier les chemins dans le DAG

3. VÃ©rifier les permissions du fichier :
   ```bash
   chmod 644 airflow/data/input.csv
   ```

### ProblÃ¨me 4 : Port dÃ©jÃ  utilisÃ©

**SymptÃ´me** : `Bind for 0.0.0.0:8080 failed: port is already allocated`

**Solutions** :
1. Changer le port dans docker-compose.yml :
   ```yaml
   ports:
     - "8081:8080"  # Au lieu de 8080:8080
   ```

2. Ou arrÃªter le service qui utilise le port :
   ```bash
   # Windows
   netstat -ano | findstr :8080
   taskkill /PID <PID> /F
   ```

---

## ğŸ“Š Architecture et Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  input.csv  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚â”€â”€â”€â”€â”€â–¶â”‚ PostgreSQL   â”‚
â”‚  (ETL Process)  â”‚      â”‚   (Storage)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   FastAPI     â”‚      â”‚  Streamlit   â”‚
            â”‚  (REST API)   â”‚      â”‚   (Visuali-  â”‚
            â”‚               â”‚      â”‚   zation)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… CritÃ¨res de RÃ©ussite du Workshop

Ã€ la fin du workshop, vous devez avoir :

1. âœ… CrÃ©Ã© les 3 Dockerfiles (Airflow, FastAPI, Streamlit)
2. âœ… CrÃ©Ã© le fichier docker-compose.yml avec tous les services
3. âœ… LancÃ© tous les conteneurs avec succÃ¨s
4. âœ… ExÃ©cutÃ© le DAG Airflow pour charger les donnÃ©es dans PostgreSQL
5. âœ… TestÃ© l'API FastAPI via Swagger
6. âœ… VisualisÃ© les donnÃ©es dans Streamlit
7. âœ… Compris les interactions entre les diffÃ©rents services

---

## ğŸ“ Concepts Docker Appris

- **Dockerfiles** : CrÃ©ation d'images personnalisÃ©es
- **Multi-stage builds** : (optionnel) Optimisation des images
- **Docker Compose** : Orchestration de services multiples
- **RÃ©seaux Docker** : Communication inter-conteneurs
- **Volumes Docker** : Persistance et partage de donnÃ©es
- **Variables d'environnement** : Configuration des conteneurs
- **Health checks** : Gestion des dÃ©pendances entre services
- **Logs et dÃ©bogage** : Diagnostiquer les problÃ¨mes

---

## ğŸ“š Ressources SupplÃ©mentaires

- [Documentation Docker](https://docs.docker.com/)
- [Documentation Docker Compose](https://docs.docker.com/compose/)
- [Documentation Airflow](https://airflow.apache.org/docs/)
- [Documentation FastAPI](https://fastapi.tiangolo.com/)
- [Documentation Streamlit](https://docs.streamlit.io/)

---

## ğŸ† DÃ©fis Bonus

Une fois le workshop terminÃ©, essayez ces dÃ©fis :

1. **SÃ©curitÃ©** : Ajouter une authentification JWT Ã  FastAPI
2. **Optimisation** : Utiliser des images multi-stage pour rÃ©duire la taille
3. **Monitoring** : Ajouter un service de monitoring (Prometheus/Grafana)
4. **CI/CD** : CrÃ©er un pipeline GitHub Actions pour build et test
5. **Scaling** : Utiliser CeleryExecutor pour Airflow
6. **Redis** : Ajouter Redis comme cache pour FastAPI

---

Bon workshop ! ğŸš€ğŸ³
